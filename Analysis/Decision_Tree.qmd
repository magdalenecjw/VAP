---
title: "Decision Tree"
author: "Habari Tanzania"
date: 11 Mar 2023
date-modified: "`r Sys.Date()`"
execute: 
  echo: true
  eval: true
  warning: false
format: html
editor: visual
---

# Load Packages and Data

```{r}
pacman::p_load(tidyverse, rpart, rpart.plot, sparkline, visNetwork, 
               caret, ranger, patchwork)
```

-   As there are too many levels within the variable Country (and Code), this variable will be removed from the regression data set.

-   Similarly, ID cannot be used as a predictor, hence, will be removed.

```{r}
df <- read_csv("data/touristdata_clean.csv")

df_analysis <- df %>% 
  select(!ID) %>% 
  select(!code) %>% 
  select(!country)
```

# Regression Tree

### Basic Regression Tree model

```{r}
anova.model <- function(min_split, complexity_parameter, max_depth) {
  rpart(total_cost ~ ., 
        data = df_analysis, 
        method = "anova", 
        control = rpart.control(minsplit = min_split, 
                                cp = complexity_parameter, 
                                maxdepth = max_depth))
  }

fit_tree <- anova.model(5, 0.001, 10)
```

### Visualising the Regression Tree

```{r}
visTree(fit_tree, edgesFontSize = 14, nodesFontSize = 16, width = "100%")

```

### Tuning of hyperparameters

```{r}
printcp(fit_tree)
```

```{r}
bestcp <- fit_tree$cptable[which.min(fit_tree$cptable[,"xerror"]),"CP"]
pruned_tree <- prune(fit_tree, cp = bestcp)

visTree(pruned_tree, edgesFontSize = 14, nodesFontSize = 16, width = "100%")
```

# Random Forest

### Splitting of data set into train vs. test data

```{r}
set.seed(1234)

trainIndex <- createDataPartition(df_analysis$total_cost, p = 0.8, 
                                  list = FALSE, 
                                  times = 1)

df_train <- df_analysis[trainIndex,]
df_test <- df_analysis[-trainIndex,]
```

### Hyperparameter Tuning and Training of Model

```{r}
##setting option for user to decide if they want to do parameter tuning
##default is simple bootstrap resampling
trctrl <- trainControl(method = "none")

##alternative is (repeated) k-fold cross-validation - user input decision on the value of k and the number of repeats
cvControl <- trainControl(##default of 10, range: 3-50
                           method = "cv",
                           number = 10)

repeatcvControl <- trainControl(##default of 10, range: 3-50
                           method = "repeatedcv",
                           number = 10,
                           ##default of 3, range: 3-10
                           repeats = 10)

##building of model
rf_model <- train(total_cost ~ ., 
                  data = df_train,
                  method = "ranger", 
                  trControl = repeatcvControl, 
            #trControl (refer to above objects created)
                  num.trees = 50, #can consider range of 5 to 200 trees
                  importance = "impurity", 
            #variable importance computation: "impurity", "permutation"
                  tuneGrid = data.frame(mtry = sqrt(ncol(df_train)),
                                        min.node.size = 5,
                                        splitrule = "variance") 
            #splitrule: "variance" (default), "extratrees", "maxstat", "beta"
            #min.node.size: default of 5 for regression trees
            #mtry: default is square root of number of variables
                  ) 
```

### Visualising of predicted vs. observed responses

```{r}
##Fit test data into the model that has been built
df_test$fit_forest <- predict(rf_model, df_test)

##Scatterplot of predicted vs. observed and Residuals scatterplot
rf_scatter <- ggplot() + 
  geom_point(aes(x = df_test$total_cost, y = df_test$fit_forest)) +
  labs(x = "Actual Total Cost", y = "Predicted Total Cost",
       title = paste0("R-squared: ", round(rf_model$finalModel$r.squared, digits=2))) + 
  theme(axis.text = element_text(size = 5),
        axis.title = element_text(size = 8),
        title = element_text(size = 8))

rf_residuals <- ggplot() + 
  geom_point(aes(x = df_test$total_cost, 
                 y = (df_test$fit_forest-df_test$total_cost)),
             col="blue3") +
  labs(y ="Residuals (Predicted-Actual)", x = "Actual Total Cost") + 
  geom_hline(yintercept = 0, col="red4", linetype = "dashed", linewidth = 0.5) + 
  theme(axis.text = element_text(size = 5),
        axis.title = element_text(size = 8))

p <- rf_scatter + rf_residuals + 
  plot_annotation(title = "Scatterplot of predicted vs. actual total cost", 
                  theme = theme(plot.title = element_text(size = 18)))

p
```

### Visualising variable importance (top 20)

```{r}
varImp(rf_model)
```

### Visualising of R-squred value vs. the number of trees

-   The original intention was to take in an input range for the number of trees and build a chart that would show a summary of how the R-squared value changes accordingly for the model trained above.

-   However, after an attempt to build the chart on the Shiny app, we realised that the app may not have sufficient memory space to handle the for loop required to build this chart. As such, this chart is dropped from the final Shiny app.

```{r}
tree_range <- 5:150

rsquared_trees <- c()

for (i in tree_range){
  rf_model <- train(total_cost ~ ., 
                  data = df_train,
                  method = "ranger", 
                  trControl = trctrl, 
                  num.trees = i,
                  importance = "impurity", 
                  tuneGrid = data.frame(mtry = sqrt(ncol(df_train)),
                                        min.node.size = 5,
                                        splitrule = "variance"))
  
  rsquared_trees <- append(rsquared_trees, rf_model$finalModel$r.squared)
}

rsquared_plot <- data.frame(tree_range, rsquared_trees)

ggplot(df = rsquared_plot) + 
  geom_point(aes(x = tree_range, y = rsquared_trees)) + 
  labs(x = "Number of trees", y = "R-squared values", 
       title = "R-squared vs. Number of Trees Plot")
```

# Insights for Shiny App

-   Random Forest will likely always produce a better baseline model than Decision Tree, even with hyperparameter tuning.

-   Given the same parameters, the model generally hits a performance plateau as the number of trees increases in the forest.

-   (Repeated) kfold Cross Validation seems to perform slightly better than Bootstrap Resampling across all diagnostic statistics when there are less trees, but difference is marginal as the number of trees increase.
